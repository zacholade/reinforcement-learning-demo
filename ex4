
Monte Carlo control (MC) has a much slower initial learning curve when compared to both SARSA and Q-learning TD approaches. This is because unlike TD approaches, MC cannot learn without reaching the terminal goal state and is what causes the more negative undiscounted return for MC when compared to SARSA and Q-learning, especially in early episodes while it is acting almost completely randomly. Additionally, its state-action pairs are only updated using the final reward, and neighbouring states are not taken into account.

Sarsa and Q-learning benefit from quicker learning when compared to Monte-Carlo as they do not need to wait for an episode to terminate to update their state-action pair value function, Q(s, a). However, SARSA unlike Q-learning, uses on-policy learning by choosing its new action, a' using the same epsilon-greedy policy that was used to choose initial action, a. Because of this difference, and the fact that we aren't reducing epsilon over time, SARSA converged on the safer optimal epsilon-greedy policy, whereas Q-learning picked its next action greedily off-policy and learnt the optimal policy (optimal path). This difference can be observed in the slightly more negative and stochastic mean undiscounted return for SARSA in all episodes. An improvement all agents could benefit from is gradually reducing epsilon over time, which would limit exploration from taking place and encourage exploitation in later episodes once convergence to an optimal policy (epsilon greedy optimal in the case of SARSA) has mostly finished.




In my improved Q-Learning approach, I implemented a combination of Dyna-Q learning and reward-based epsilon decay, where the latter reduces exploration over time and in turn, encourages the agent to act more greedily based on information it has since learnt. In my implementation, the rate of decay is based off various factors including a decay rate to multiply epsilon by after each episode, a reward threshold which must be met before starting decay, and a reward increment which increases the reward threshold over time. The more negative discounted reward value in early episodes can be explained by the high starting epsilon value which encourages early learning over exploitation. This epsilon ends up tending towards 0, whereby exploitation of knowledge learnt is exploited and is therefore more representative the optimal policy.

Dyna-Q is the implementation of both direct reinforcement learning (what the regular one-step tabular Q-learning does) and model learning, which has the benefit of learning not only from real experience, but also simulated experience. In the model learning phase, we loop n amount of times, randomly picking a state-action pair we have previously explored and update our Q value function with the next state and reward as its prediction.


While these methods exceeded all my initial expectations, I would have additionally liked to experiment with implementing prioritised sweeping, whereby instead of sampling randomly from previously experienced state-action pairs, we explore these pairs in order of priority. This method has the obvious benefit of being more efficient, and in general, we prioritise backwards from the terminal goal state (backward focused), as these states have the greatest reward.



